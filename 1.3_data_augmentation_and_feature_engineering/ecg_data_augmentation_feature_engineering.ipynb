{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a91a0df9",
   "metadata": {},
   "source": [
    "# Task 1.3 â€“ ECG - data augmentation and feature engineering \n",
    "Augment the data and featurer engineer with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd92d7a",
   "metadata": {},
   "source": [
    "1.3 Data Augmentation and Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88ef1cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsdaten: 4943 Zeitreihen\n",
      "Validierungsmenge: 1236 Beispiele\n",
      "Train-class-distribution: Counter({0: 2910, 2: 1412, 1: 439, 3: 182})\n",
      "Val-class-distribution: Counter({0: 728, 2: 353, 1: 110, 3: 45})\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "with open(\"../data/split_data.pkl\", \"rb\") as f:\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = pickle.load(f)\n",
    "\n",
    "print(f\"Trainingsdaten: {len(X_train_split)} Zeitreihen\")\n",
    "print(f\"Validierungsmenge: {len(X_val_split)} Beispiele\")\n",
    "print(\"Train-class-distribution:\", Counter(y_train_split))\n",
    "print(\"Val-class-distribution:\", Counter(y_val_split))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd1ff7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mfsafi/Desktop/AMLS Project/ehsan_AMLS_Exercise/.venv/lib/python3.9/site-packages/torch/functional.py:730: UserWarning: A window was not provided. A rectangular window will be applied,which is known to cause spectral leakage. Other windows such as torch.hann_window or torch.hamming_window are recommended to reduce spectral leakage.To suppress this warning and use a rectangular window, explicitly set `window=torch.ones(n_fft, device=<device>)`. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/SpectralOps.cpp:842.)\n",
      "  return _VF.stft(  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4943, 33, 1142])\n",
      "torch.Size([1236, 33, 1143])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class STFTLayer(torch.nn.Module):\n",
    "    def __init__(self, n_fft=64, hop_length=16):\n",
    "        super().__init__()\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        stft_tensors = []\n",
    "        max_time_steps = 0\n",
    "\n",
    "        # First pass: compute STFTs and find max time dimension\n",
    "        for signal in x:\n",
    "            signal_tensor = torch.tensor(signal, dtype=torch.float32)\n",
    "            stft = torch.stft(signal_tensor, n_fft=self.n_fft, hop_length=self.hop_length,\n",
    "                              return_complex=True)\n",
    "            magnitude = stft.abs()\n",
    "            stft_tensors.append(magnitude)\n",
    "            max_time_steps = max(max_time_steps, magnitude.shape[1])\n",
    "\n",
    "        # Second pass: pad all to the same shape\n",
    "        padded = []\n",
    "        for m in stft_tensors:\n",
    "            pad_size = max_time_steps - m.shape[1]\n",
    "            m_padded = F.pad(m, (0, pad_size))  # pad last dim (time) to the right\n",
    "            padded.append(m_padded)\n",
    "\n",
    "        return torch.stack(padded)\n",
    "    \n",
    "stft_layer = STFTLayer(n_fft=64, hop_length=16)\n",
    "X_train_stft = stft_layer(X_train_split)\n",
    "X_val_stft = stft_layer(X_val_split)\n",
    "\n",
    "y_train = y_train_split\n",
    "y_val = y_val_split\n",
    "\n",
    "print(X_train_stft.shape)\n",
    "print(X_val_stft.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d84e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([32, 1, 33, 1142])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../1.1_dataset_exploration/src\")  # adjust path if needed\n",
    "from dataset import create_spectrogram_dataloaders\n",
    "\n",
    "# Create DataLoaders with augmentations for training\n",
    "train_loader, val_loader = create_spectrogram_dataloaders(\n",
    "    X_train_stft, y_train, X_val_stft, y_val, batch_size=32, augment=True\n",
    ")\n",
    "\n",
    "# Check one batch (optional)\n",
    "for xb, yb in train_loader:\n",
    "    print(\"Batch shape:\", xb.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b5775e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=200.1508, Train Acc=0.5141, Val Loss=47.3291, Val Acc=0.5890\n",
      "Epoch 2: Train Loss=186.0347, Train Acc=0.5642, Val Loss=44.2877, Val Acc=0.5890\n",
      "Epoch 3: Train Loss=176.2215, Train Acc=0.5675, Val Loss=42.4021, Val Acc=0.5890\n",
      "Epoch 4: Train Loss=172.3179, Train Acc=0.5735, Val Loss=41.2844, Val Acc=0.5890\n",
      "Epoch 5: Train Loss=170.8521, Train Acc=0.5713, Val Loss=40.9420, Val Acc=0.5914\n",
      "Epoch 6: Train Loss=167.8655, Train Acc=0.5733, Val Loss=40.2210, Val Acc=0.5906\n",
      "Epoch 7: Train Loss=167.0938, Train Acc=0.5774, Val Loss=40.2273, Val Acc=0.5906\n",
      "Epoch 8: Train Loss=166.8392, Train Acc=0.5719, Val Loss=40.3340, Val Acc=0.5914\n",
      "Epoch 9: Train Loss=165.7460, Train Acc=0.5733, Val Loss=40.0386, Val Acc=0.5914\n",
      "Epoch 10: Train Loss=165.5831, Train Acc=0.5701, Val Loss=39.6227, Val Acc=0.5930\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import your model from model.py (adjust path if needed)\n",
    "from model import ECGCNN\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model, train_loader, val_loader, epochs=10, lr=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                outputs = model(X_val)\n",
    "                loss = criterion(outputs, y_val)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += y_val.size(0)\n",
    "                val_correct += (predicted == y_val).sum().item()\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, \"\n",
    "              f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "# Instantiate and train the model\n",
    "cnn_model = ECGCNN()\n",
    "train_model(cnn_model, train_loader, val_loader, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff977c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test samples: 2649\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import struct\n",
    "\n",
    "def read_zip_binary(path):\n",
    "    ragged_array = []\n",
    "    with zipfile.ZipFile(path, 'r') as zf:\n",
    "        inner_path = path.split(\"/\")[-1].split(\".\")[0]\n",
    "        with zf.open(f'{inner_path}.bin', 'r') as r:\n",
    "            while True:\n",
    "                size_bytes = r.read(4)\n",
    "                if not size_bytes:\n",
    "                    break\n",
    "                sub_array_size = struct.unpack('i', size_bytes)[0]\n",
    "                sub_array = list(struct.unpack(f'{sub_array_size}h', r.read(sub_array_size * 2)))\n",
    "                ragged_array.append(sub_array)\n",
    "    return ragged_array\n",
    "\n",
    "# Read test data from zip\n",
    "X_test_raw = read_zip_binary(\"../1.2_modeling_and_tuning/data/X_test.zip\")\n",
    "print(\"Loaded test samples:\", len(X_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25c8a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_stft = stft_layer(X_test_raw)\n",
    "X_test_stft = X_test_stft.unsqueeze(1)  # Add channel dim for CNN (B, 1, Freq, Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a485e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "cnn_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_test_stft = X_test_stft.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = cnn_model(X_test_stft)\n",
    "    predictions = torch.argmax(outputs, dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d517942b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to augment.csv âœ…\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(predictions, columns=[\"label\"])\n",
    "df.to_csv(\"augment.csv\", index=False, header=False)\n",
    "print(\"Saved predictions to augment.csv âœ…\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
