{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13d65fed",
   "metadata": {},
   "source": [
    "# Task 1.2 – Modeling and Tuning (ECG Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207e2b30",
   "metadata": {},
   "source": [
    "Read training-data (ECG-time series) as pickle from task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d84b3726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsdaten: 4945 Zeitreihen\n",
      "Validierungsmenge: 1234 Beispiele\n",
      "Train-class-distribution: Counter({0: 2911, 2: 1412, 1: 440, 3: 182})\n",
      "Val-class-distribution: Counter({0: 727, 2: 353, 1: 109, 3: 45})\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "# Load the dataset from task 1\n",
    "with open(\"../data/split_data.pkl\", \"rb\") as f:\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = pickle.load(f)\n",
    "\n",
    "# X_train_split: Trainingsdaten\n",
    "# X_val_split: Validierungsdaten\n",
    "# y_train_split: Labels für Trainingsdaten\n",
    "# y_val_split: Labels für Validierungsdaten\n",
    "\n",
    "print(f\"Trainingsdaten: {len(X_train_split)} Zeitreihen\")\n",
    "print(f\"Validierungsmenge: {len(X_val_split)} Beispiele\")\n",
    "\n",
    "# class distribution\n",
    "print(\"Train-class-distribution:\", Counter(y_train_split))\n",
    "print(\"Val-class-distribution:\", Counter(y_val_split))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6981b113",
   "metadata": {},
   "source": [
    "Padding of all training time series (= adding 0 to the ECG-time series to make them \n",
    "evenly sized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "598f17d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training and validation sequences: 18286\n",
      "Shape of padded training data: (4945, 18286)\n",
      "Shape of padded validation data: (1234, 18286)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_sequences(sequences, max_len=None):\n",
    "    if not max_len:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "    return np.array([np.pad(seq, (0, max_len - len(seq))) for seq in sequences])\n",
    "\n",
    "# Check the maximum length of training and validation sequences\n",
    "max_len = max(max(len(seq) for seq in X_train_split),\n",
    "              max(len(seq) for seq in X_val_split))\n",
    "\n",
    "print(\"Maximum length of training and validation sequences:\", max_len)\n",
    "\n",
    "# Pad the sequences to the same length\n",
    "X_train_split = pad_sequences(X_train_split, max_len)\n",
    "X_val_split = pad_sequences(X_val_split, max_len)\n",
    "\n",
    "# Check the shape of the padded sequences\n",
    "print(\"Shape of padded training data:\", X_train_split.shape)\n",
    "print(\"Shape of padded validation data:\", X_val_split.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff5bf57",
   "metadata": {},
   "source": [
    "Convert 1D-NumPy-Arrays into 2D-Tensor with short-time Fourier transform (STFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db8f228c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of STFT training data: torch.Size([4945, 129, 143])\n",
      "Shape of STFT validation data: torch.Size([1234, 129, 143])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ../data does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of STFT validation data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_val_stft\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Save the STFT results\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX_val_stft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_stft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_val_split\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_split\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/val_data.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    850\u001b[0m         _save(\n\u001b[1;32m    851\u001b[0m             obj,\n\u001b[1;32m    852\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    856\u001b[0m         )\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/serialization.py:716\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/serialization.py:687\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory ../data does not exist."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def apply_stft_numpy(X, n_fft=256, hop_length=128):\n",
    "    \"\"\"\n",
    "    Wandelt ein NumPy-Array mit gepaddeten Zeitreihen in STFT-Spektren um.\n",
    "    \n",
    "    Args:\n",
    "        data_np (np.ndarray): Input (n_samples, sequence_length)\n",
    "        n_fft (int): Größe des STFT-Fensters\n",
    "        hop_length (int): Schrittweite der Fenster\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: STFT-Magnitude-Spektren (n_samples, freq_bins, time_steps)\n",
    "    \"\"\"\n",
    "    # convert to torch tensor\n",
    "    data_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "    # window function for STFT\n",
    "    window = torch.hann_window(n_fft)\n",
    "\n",
    "    # empty list to store STFT results\n",
    "    stft_results = []\n",
    "\n",
    "    # iterate over each time series\n",
    "    for i in range(data_tensor.shape[0]):\n",
    "        # compute STFT\n",
    "        stft = torch.stft(\n",
    "            data_tensor[i], \n",
    "            n_fft=n_fft, \n",
    "            hop_length=hop_length, \n",
    "            window=window, \n",
    "            return_complex=True\n",
    "            )\n",
    "        \n",
    "        # compute magnitude\n",
    "        stft_magnitude = torch.abs(stft)\n",
    "        \n",
    "        # append to results\n",
    "        stft_results.append(stft_magnitude)\n",
    "\n",
    "        # convert into 3D tensor\n",
    "        stft_tensor = torch.stack(stft_results)\n",
    "        \n",
    "    return stft_tensor\n",
    "    \n",
    "# Apply STFT to the training and validation data\n",
    "X_train_stft = apply_stft_numpy(X_train_split)  # Output: Tensor mit Shape (4943, freq_bins, time_steps)\n",
    "X_val_stft = apply_stft_numpy(X_val_split)\n",
    "\n",
    "# Check the shape of the STFT results\n",
    "print(\"Shape of STFT training data:\", X_train_stft.shape)\n",
    "print(\"Shape of STFT validation data:\", X_val_stft.shape)\n",
    "\n",
    "# Save the STFT results\n",
    "torch.save({'X_val_stft': X_val_stft, 'y_val_split': y_val_split}, '../data/val_data.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10afb75d",
   "metadata": {},
   "source": [
    "Convert tensors into DataLoader objects for the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf31f189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape (X): torch.Size([32, 1, 129, 143])\n",
      "Batch shape (y): torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a custom Dataset class for the STFT data\n",
    "class ECGSpectrogramDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.unsqueeze(1)  # füge Channel-Dimension hinzu → (N, 1, Freq, Time)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# function to create DataLoader objects for training and validation\n",
    "def create_spectrogram_dataloaders(X_train_stft, y_train, X_val_stft, y_val, batch_size=32):\n",
    "    \"\"\"\n",
    "    Erstellt trainierbare DataLoader aus STFT-Tensoren und Label-Listen.\n",
    "\n",
    "    Args:\n",
    "        X_train_stft (torch.Tensor): Trainingsdaten (N_train, Freq, Time)\n",
    "        y_train (List[int]): Trainingslabels\n",
    "        X_val_stft (torch.Tensor): Validierungsdaten (N_val, Freq, Time)\n",
    "        y_val (List[int]): Validierungslabels\n",
    "        batch_size (int): Größe der Batches\n",
    "\n",
    "    Returns:\n",
    "        train_loader, val_loader: DataLoader-Objekte\n",
    "    \"\"\"\n",
    "    train_dataset = ECGSpectrogramDataset(X_train_stft, y_train)\n",
    "    val_dataset = ECGSpectrogramDataset(X_val_stft, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Create DataLoader objects for training and validation\n",
    "train_loader, val_loader = create_spectrogram_dataloaders(\n",
    "    X_train_stft, y_train_split, X_val_stft, y_val_split\n",
    ")\n",
    "\n",
    "# Check the shape of a batch from the DataLoader and the labels\n",
    "X_batch, y_batch = next(iter(train_loader))\n",
    "print(\"Batch shape (X):\", X_batch.shape)\n",
    "print(\"Batch shape (y):\", y_batch.shape)\n",
    "#print(\"Labels im Batch:\", X_batch.tolist())\n",
    "#print(\"Labels im Batch:\", y_batch.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e9b42",
   "metadata": {},
   "source": [
    "Definition and training of CNN-model (Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dcfe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# from model import ECGCNN\n",
    "\n",
    "# # method to train the model \n",
    "# def train_model(model, train_loader, val_loader, epochs=10, lr=0.001):\n",
    "    \n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.to(device)\n",
    "\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         train_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "#         for X_batch, y_batch in train_loader:\n",
    "#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(X_batch)\n",
    "#             loss = criterion(outputs, y_batch)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             train_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += y_batch.size(0)\n",
    "#             correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "#         train_acc = correct / total\n",
    "\n",
    "#         # evaluate on validation set\n",
    "#         model.eval()\n",
    "#         val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "#         with torch.no_grad():\n",
    "#             for X_val, y_val in val_loader:\n",
    "#                 X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "#                 outputs = model(X_val)\n",
    "#                 loss = criterion(outputs, y_val)\n",
    "#                 val_loss += loss.item()\n",
    "#                 _, predicted = torch.max(outputs, 1)\n",
    "#                 val_total += y_val.size(0)\n",
    "#                 val_correct += (predicted == y_val).sum().item()\n",
    "\n",
    "#         val_acc = val_correct / val_total\n",
    "\n",
    "#         # print the training and validation loss and accuracy\n",
    "#         print(f\"Epoch {epoch+1}: \"\n",
    "#               f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, \"\n",
    "#               f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "\n",
    "# # only once for testing the model architecture\n",
    "# # X_batch, _ = next(iter(train_loader))\n",
    "# # cnn_model = ECGCNN()\n",
    "# # cnn_model(X_batch)  # ruft automatisch forward() auf\n",
    "\n",
    "# # instantiate the model and train it   \n",
    "# cnn_model = ECGCNN()\n",
    "\n",
    "# # train the model\n",
    "# train_model(cnn_model, train_loader, val_loader, epochs=10)\n",
    "\n",
    "# # # Modell speichern\n",
    "# # torch.save(cnn_model.state_dict(), \"../models/cnn_model_wt_tune.pth\")\n",
    "\n",
    "# # load the model\n",
    "# # cnn_model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "# # delete the model to free up memory\n",
    "# # del cnn_model\n",
    "\n",
    "# # Check the number of parameters in the model\n",
    "# print(\"Anzahl der Modell-Parameter:\", sum(p.numel() for p in cnn_model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ff382",
   "metadata": {},
   "source": [
    "Tuning approch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b59ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainiere mit Parametern: {'lr': 0.001, 'weight_decay': 0.0001, 'optimizer_type': 'adam', 'epochs': 10}\n"
     ]
    }
   ],
   "source": [
    "# import torch.optim as optim\n",
    "# from itertools import product\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Trainings- & Evaluationsfunktion\n",
    "# def train_and_eval(model, train_loader, val_loader, lr, weight_decay, optimizer_type, epochs=10):\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     if optimizer_type == \"adam\":\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#     elif optimizer_type == \"sgd\":\n",
    "#         optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "#     model.to(device)\n",
    "#     best_acc = 0\n",
    "#     val_loss_final = 0.0\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         for X_batch, y_batch in train_loader:\n",
    "#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(X_batch)\n",
    "#             loss = criterion(outputs, y_batch)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         # Evaluation\n",
    "#         model.eval()\n",
    "#         val_correct, val_total, val_loss = 0, 0, 0.0\n",
    "#         with torch.no_grad():\n",
    "#             for X_val, y_val in val_loader:\n",
    "#                 X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "#                 outputs = model(X_val)\n",
    "#                 loss = criterion(outputs, y_val)\n",
    "#                 val_loss += loss.item()\n",
    "#                 _, predicted = torch.max(outputs, 1)\n",
    "#                 val_total += y_val.size(0)\n",
    "#                 val_correct += (predicted == y_val).sum().item()\n",
    "\n",
    "#         val_acc = val_correct / val_total\n",
    "#         val_loss_final = val_loss / len(val_loader)\n",
    "\n",
    "#         if val_acc > best_acc:\n",
    "#             best_acc = val_acc\n",
    "#             filename = f\"best_lr{lr}_opt{weight_decay}.pth\"\n",
    "#             torch.save(model.state_dict(), f\"../models/tuning_approach_without_weights{filename}\")\n",
    "\n",
    "#     return best_acc, val_loss_final\n",
    "\n",
    "# # Hyperparameter-Raster\n",
    "# param_grid = {\n",
    "#     'lr': [0.001],\n",
    "#     'weight_decay': [1e-4],\n",
    "#     'optimizer_type': ['adam'],\n",
    "#     'epochs': [10]\n",
    "# }\n",
    "\n",
    "# results = []\n",
    "\n",
    "# # Tuning Loop\n",
    "# for params in product(*param_grid.values()):\n",
    "#     param_dict = dict(zip(param_grid.keys(), params))\n",
    "#     print(f\"\\nTrainiere mit Parametern: {param_dict}\")\n",
    "\n",
    "#     #model = ECGCNN()\n",
    "\n",
    "#     val_acc, val_loss = train_and_eval(model, train_loader, val_loader, **param_dict)\n",
    "\n",
    "#     param_dict[\"val_accuracy\"] = val_acc\n",
    "#     param_dict[\"val_loss\"] = val_loss\n",
    "#     results.append(param_dict)\n",
    "\n",
    "#     #del model\n",
    "#     torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b51fe",
   "metadata": {},
   "source": [
    "Tuning approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508bab57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainiere mit Parametern: {'lr': 0.001, 'weight_decay': 0.01, 'optimizer_type': 'adam', 'epochs': 10}\n",
      "\n",
      "Trainiere mit Parametern: {'lr': 0.001, 'weight_decay': 0.001, 'optimizer_type': 'adam', 'epochs': 10}\n",
      "\n",
      "Trainiere mit Parametern: {'lr': 0.001, 'weight_decay': 0.0001, 'optimizer_type': 'adam', 'epochs': 10}\n",
      "\n",
      "Trainiere mit Parametern: {'lr': 0.001, 'weight_decay': 1e-05, 'optimizer_type': 'adam', 'epochs': 10}\n",
      "\n",
      "Trainiere mit Parametern: {'lr': 0.001, 'weight_decay': 0, 'optimizer_type': 'adam', 'epochs': 10}\n"
     ]
    }
   ],
   "source": [
    "# import torch.optim as optim\n",
    "# from itertools import product\n",
    "# import torch.nn as nn\n",
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# class_counts = np.bincount(y_train_split)\n",
    "# class_weights = 1. / class_counts\n",
    "# class_weights = class_weights / class_weights.sum()\n",
    "# weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# def train_and_eval(model, train_loader, val_loader, lr, weight_decay, optimizer_type, epochs=10):\n",
    "#     criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "#     if optimizer_type == \"adam\":\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#     elif optimizer_type == \"sgd\":\n",
    "#         optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "#     model.to(device)\n",
    "#     best_acc = 0\n",
    "#     val_loss_final = 0.0\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         for X_batch, y_batch in train_loader:\n",
    "#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(X_batch)\n",
    "#             loss = criterion(outputs, y_batch)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         # Evaluation\n",
    "#         model.eval()\n",
    "#         val_correct, val_total, val_loss = 0, 0, 0.0\n",
    "#         with torch.no_grad():\n",
    "#             for X_val, y_val in val_loader:\n",
    "#                 X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "#                 outputs = model(X_val)\n",
    "#                 loss = criterion(outputs, y_val)\n",
    "#                 val_loss += loss.item()\n",
    "#                 _, predicted = torch.max(outputs, 1)\n",
    "#                 val_total += y_val.size(0)\n",
    "#                 val_correct += (predicted == y_val).sum().item()\n",
    "\n",
    "#         val_acc = val_correct / val_total\n",
    "#         val_loss_final = val_loss / len(val_loader)\n",
    "\n",
    "#         if val_acc > best_acc:\n",
    "#             best_acc = val_acc\n",
    "#             filename = f\"best_lr{lr}_opt{weight_decay}.pth\"\n",
    "#             torch.save(model.state_dict(), f\"../models/tuning_approach_2/{filename}\")\n",
    "\n",
    "#     return best_acc, val_loss_final\n",
    "\n",
    "# # Hyperparameter-Raster\n",
    "# param_grid = {\n",
    "#     'lr': [0.001],\n",
    "#     'weight_decay': [1e-2, 1e-3, 1e-4, 1e-5, 0],\n",
    "#     'optimizer_type': ['adam'],\n",
    "#     'epochs': [10]\n",
    "# }\n",
    "\n",
    "# results = []\n",
    "\n",
    "# # Tuning Loop\n",
    "# for params in product(*param_grid.values()):\n",
    "#     param_dict = dict(zip(param_grid.keys(), params))\n",
    "#     print(f\"\\nTrainiere mit Parametern: {param_dict}\")\n",
    "\n",
    "#     #model = ECGCNN()\n",
    "\n",
    "#     val_acc, val_loss = train_and_eval(model, train_loader, val_loader, **param_dict)\n",
    "\n",
    "#     param_dict[\"val_accuracy\"] = val_acc\n",
    "#     param_dict[\"val_loss\"] = val_loss\n",
    "#     results.append(param_dict)\n",
    "\n",
    "#     del model\n",
    "#     torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
